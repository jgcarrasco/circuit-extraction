{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from typing import Optional, Tuple, Any, Union\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention, GPT2Model\n",
    "\n",
    "from utils import get_data\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained the circuits, it is time to truly obtain a pruned model. We will do it directly on the HuggingFace implementation, as we don't need TransformerLens now and we actually care about performance and not interpretability right now. We will start by pruning zero ablated models, as we don't have to take care of adding any bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"acronyms\"\n",
    "ABLATION = \"zero\"\n",
    "INCLUDE_MLPS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", add_bos_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", output_hidden_states=False, use_cache=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "n_patching = 100\n",
    "n_val = 100\n",
    "task = \"acronyms\"\n",
    "\n",
    "data = get_data(n_patching=n_patching, n_val=n_val, task=task)\n",
    "\n",
    "patching_tokens = data[\"patching_tokens\"] \n",
    "patching_answer_tokens = data[\"patching_answer_tokens\"] \n",
    "patching_logits = data[\"patching_logits\"] \n",
    "patching_cache = data[\"patching_cache\"]\n",
    "\n",
    "val_tokens = data[\"val_tokens\"] \n",
    "val_answer_tokens = data[\"val_answer_tokens\"]\n",
    "val_logits = data[\"val_logits\"]\n",
    "val_cache = data[\"val_cache\"]\n",
    "\n",
    "gt_circuit = data[\"gt_circuit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, val_tokens, val_answer_tokens, task=\"acronyms\"):\n",
    "    if task == \"acronyms\":\n",
    "        return (model(val_tokens)[\"logits\"][:, -1].argmax(-1) == val_answer_tokens[:, -1]).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionIdentity(nn.Module):\n",
    "    \"\"\"\n",
    "    Placeholder for the GPT2Attention layer, literally does nothing\n",
    "    \"\"\"\n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n",
    "        return (torch.zeros_like(hidden_states), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_dict(attn_heads, n_heads=12, n_layers=12):\n",
    "    \"\"\"\n",
    "    Given a list of the attn heads of the circuit, returns\n",
    "    a dictionary heads_to_prune[layer] = [head, ...] with\n",
    "    every attention head outside of the circuit.\n",
    "    \"\"\"\n",
    "    heads_to_prune = {}\n",
    "    for layer in range(n_layers):\n",
    "        heads_to_prune[layer] = [head for head in range(n_heads)]\n",
    "\n",
    "    for layer, head in attn_heads:\n",
    "        heads_to_prune[layer].remove(head)\n",
    "        \n",
    "    return heads_to_prune\n",
    "\n",
    "def get_attn_layers_to_prune(heads_to_prune, n_heads=12):\n",
    "    \"\"\"\n",
    "    If heads_to_prune[layer] contains every head of the attention layer,\n",
    "    we directly remove the complete layer instead of every separate head.\n",
    "    \"\"\"\n",
    "    attn_layers_to_prune = []\n",
    "    for layer in heads_to_prune.keys():\n",
    "        if len(heads_to_prune[layer]) == n_heads:\n",
    "            attn_layers_to_prune.append(layer)\n",
    "    for layer in attn_layers_to_prune:\n",
    "        del heads_to_prune[layer]\n",
    "    return heads_to_prune, attn_layers_to_prune\n",
    "\n",
    "def prune_attn_layers(self: GPT2Model, layers):\n",
    "    for layer in layers:\n",
    "        self.h[layer].attn = AttentionIdentity()\n",
    "\n",
    "GPT2Model.prune_attn_layers = prune_attn_layers\n",
    "\n",
    "\n",
    "attn_heads = gt_circuit\n",
    "heads_to_prune = list_to_dict(attn_heads)\n",
    "heads_to_prune, attn_layers_to_prune = get_attn_layers_to_prune(heads_to_prune)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", output_hidden_states=False, use_cache=False).cuda()\n",
    "model.transformer._prune_heads(heads_to_prune)\n",
    "model.transformer.prune_attn_layers(attn_layers_to_prune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.hook_q_input', 'blocks.0.hook_k_input', 'blocks.0.hook_v_input', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.attn.hook_result', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.hook_mlp_in', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.hook_q_input', 'blocks.1.hook_k_input', 'blocks.1.hook_v_input', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.attn.hook_result', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.hook_mlp_in', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.hook_q_input', 'blocks.2.hook_k_input', 'blocks.2.hook_v_input', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.attn.hook_result', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.hook_mlp_in', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.hook_q_input', 'blocks.3.hook_k_input', 'blocks.3.hook_v_input', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.attn.hook_result', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.hook_mlp_in', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.hook_q_input', 'blocks.4.hook_k_input', 'blocks.4.hook_v_input', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.attn.hook_result', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.hook_mlp_in', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.hook_q_input', 'blocks.5.hook_k_input', 'blocks.5.hook_v_input', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.attn.hook_result', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.hook_mlp_in', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.hook_q_input', 'blocks.6.hook_k_input', 'blocks.6.hook_v_input', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.attn.hook_result', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.hook_mlp_in', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.hook_q_input', 'blocks.7.hook_k_input', 'blocks.7.hook_v_input', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.attn.hook_result', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.hook_mlp_in', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.hook_q_input', 'blocks.8.hook_k_input', 'blocks.8.hook_v_input', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.attn.hook_result', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.hook_mlp_in', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.hook_q_input', 'blocks.9.hook_k_input', 'blocks.9.hook_v_input', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.attn.hook_result', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.hook_mlp_in', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.hook_q_input', 'blocks.10.hook_k_input', 'blocks.10.hook_v_input', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.attn.hook_result', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.hook_mlp_in', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.hook_q_input', 'blocks.11.hook_k_input', 'blocks.11.hook_v_input', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.attn.hook_result', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.hook_mlp_in', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patching_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9399999976158142 -> 0.0\n"
     ]
    }
   ],
   "source": [
    "def prune_gpt2_hf_model(model, attn_heads, ablation_scheme=\"mean\", patching_cache=None):\n",
    "    \"\"\"\n",
    "    Truly remove the attention heads and MLPs on a GPT-2 model. Note\n",
    "    that we can either remove attention heads, MLPs, or both.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - `model`: \n",
    "    - `attn_heads`: List [[layer, head]] containing the attention heads of the circuit\n",
    "    Returns\n",
    "    -------\n",
    "    Pruned model\n",
    "    \"\"\"\n",
    "    if ablation_scheme == \"mean\":\n",
    "        assert patching_cache is not None, \"You need to provide the cached activations when mean patching\"\n",
    "\n",
    "    heads_to_prune = list_to_dict(attn_heads)\n",
    "    heads_to_prune, attn_layers_to_prune = get_attn_layers_to_prune(heads_to_prune)\n",
    "    \n",
    "    model.transformer._prune_heads(heads_to_prune)\n",
    "    model.transformer.prune_attn_layers(attn_layers_to_prune)\n",
    "    return model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", output_hidden_states=False, use_cache=False).cuda()\n",
    "acc_before = compute_accuracy(model, val_tokens, val_answer_tokens)\n",
    "model = prune_gpt2_hf_model(model, gt_circuit, ablation_scheme=\"mean\", patching_cache=patching_cache)\n",
    "acc_after = compute_accuracy(model, val_tokens, val_answer_tokens)\n",
    "print(acc_before, \"->\", acc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"logs/{TASK}_{ABLATION}{'_mlp' if INCLUDE_MLPS else ''}.pkl\", \"rb\") as handle:\n",
    "    log = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>acc</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.94</td>\n",
       "      <td>124439808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.94</td>\n",
       "      <td>124439808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.94</td>\n",
       "      <td>124439808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.94</td>\n",
       "      <td>124439808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.94</td>\n",
       "      <td>124439808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.94</td>\n",
       "      <td>124439808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.94</td>\n",
       "      <td>124439808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.94</td>\n",
       "      <td>123849408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.89</td>\n",
       "      <td>122275008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.92</td>\n",
       "      <td>118732608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.90</td>\n",
       "      <td>113812608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.005796</td>\n",
       "      <td>0.95</td>\n",
       "      <td>109483008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.012743</td>\n",
       "      <td>0.84</td>\n",
       "      <td>105940608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.028014</td>\n",
       "      <td>0.59</td>\n",
       "      <td>102199104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.061585</td>\n",
       "      <td>0.25</td>\n",
       "      <td>100821504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.135388</td>\n",
       "      <td>0.00</td>\n",
       "      <td>99442368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.297635</td>\n",
       "      <td>0.00</td>\n",
       "      <td>97275264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.654319</td>\n",
       "      <td>0.00</td>\n",
       "      <td>96288960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.438450</td>\n",
       "      <td>0.00</td>\n",
       "      <td>96288960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.162278</td>\n",
       "      <td>0.00</td>\n",
       "      <td>96091392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold   acc       size\n",
       "0    0.000001  0.94  124439808\n",
       "1    0.000002  0.94  124439808\n",
       "2    0.000005  0.94  124439808\n",
       "3    0.000011  0.94  124439808\n",
       "4    0.000023  0.94  124439808\n",
       "5    0.000051  0.94  124439808\n",
       "6    0.000113  0.94  124439808\n",
       "7    0.000248  0.94  123849408\n",
       "8    0.000546  0.89  122275008\n",
       "9    0.001199  0.92  118732608\n",
       "10   0.002637  0.90  113812608\n",
       "11   0.005796  0.95  109483008\n",
       "12   0.012743  0.84  105940608\n",
       "13   0.028014  0.59  102199104\n",
       "14   0.061585  0.25  100821504\n",
       "15   0.135388  0.00   99442368\n",
       "16   0.297635  0.00   97275264\n",
       "17   0.654319  0.00   96288960\n",
       "18   1.438450  0.00   96288960\n",
       "19   3.162278  0.00   96091392"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = []\n",
    "\n",
    "for threshold in sorted(log.keys()):\n",
    "    attn_heads = log[threshold][0]\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", output_hidden_states=False, use_cache=False).cuda()\n",
    "    model = prune_gpt2_hf_model(model, attn_heads)\n",
    "    acc = compute_accuracy(model, val_tokens, val_answer_tokens)\n",
    "    size = model.num_parameters()\n",
    "    df.append([threshold, acc, size])\n",
    "\n",
    "df = pd.DataFrame(df, columns=[\"threshold\", \"acc\", \"size\"])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mech_interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
