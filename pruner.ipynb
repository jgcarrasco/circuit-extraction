{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgcarrasco/.virtualenvs/mech_interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "from einops import einsum, rearrange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "\n",
    "from utils import get_data, compute_logit_diff_acronym\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_patching = 100\n",
    "n_val = 100\n",
    "task = \"acronyms\"\n",
    "\n",
    "data = get_data(n_patching=n_patching, n_val=n_val, task=task)\n",
    "\n",
    "model = data[\"model\"]\n",
    "\n",
    "patching_tokens = data[\"patching_tokens\"] \n",
    "patching_answer_tokens = data[\"patching_answer_tokens\"] \n",
    "patching_logits = data[\"patching_logits\"] \n",
    "patching_cache = data[\"patching_cache\"]\n",
    "\n",
    "val_tokens = data[\"val_tokens\"] \n",
    "val_answer_tokens = data[\"val_answer_tokens\"]\n",
    "val_logits = data[\"val_logits\"]\n",
    "val_cache = data[\"val_cache\"]\n",
    "\n",
    "gt_circuit = data[\"gt_circuit\"]\n",
    "\n",
    "del model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", output_hidden_states=False, use_cache=False).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_mean_attn_layer_activations(model, patching_tokens):\n",
    "    mean_attn_layer_activations = torch.zeros(model.config.n_layer, patching_tokens.shape[-1], model.config.n_embd)\n",
    "\n",
    "    def _cache_mean_attn_layer_activations(module, input, output, layer_idx):\n",
    "        mean_attn_layer_activations[layer_idx] = output[0].mean(0)\n",
    "        return None\n",
    "\n",
    "    for layer in range(model.config.n_layer):\n",
    "        # Gather the activations for that layer\n",
    "        hook_fn = partial(_cache_mean_attn_layer_activations, layer_idx=layer)\n",
    "        hook = model.transformer.h[layer].attn.register_forward_hook(hook_fn)\n",
    "        model(patching_tokens)\n",
    "        hook.remove()\n",
    "    return mean_attn_layer_activations\n",
    "\n",
    "\n",
    "def cache_mean_mlp_activations(model, patching_tokens):\n",
    "    mean_mlp_activations = torch.zeros(model.config.n_layer, patching_tokens.shape[-1], model.config.n_embd)\n",
    "\n",
    "    def _cache_mean_mlp_activations(module, input, output, layer_idx):\n",
    "        mean_mlp_activations[layer_idx] = output.mean(0)\n",
    "        return None\n",
    "\n",
    "    for layer in range(model.config.n_layer):\n",
    "        # Gather the activations for that layer\n",
    "        hook_fn = partial(_cache_mean_mlp_activations, layer_idx=layer)\n",
    "        hook = model.transformer.h[layer].mlp.register_forward_hook(hook_fn)\n",
    "        model(patching_tokens)\n",
    "        hook.remove()\n",
    "    return mean_mlp_activations\n",
    "\n",
    "\n",
    "def cache_mean_head_activations(model, patching_tokens):\n",
    "    d_head = int(model.config.n_embd / model.config.n_head)\n",
    "\n",
    "\n",
    "    mean_head_activations = torch.zeros(model.config.n_layer, model.config.n_head, patching_tokens.shape[-1], model.config.n_embd)\n",
    "    attn_layer_biases = torch.zeros(model.config.n_layer, model.config.n_embd)\n",
    "\n",
    "    def _cache_mean_head_activations(c_proj, input, output, layer_idx):\n",
    "        h = input[0]\n",
    "        batch_size, seq_len, d_model = h.size()\n",
    "        h = h.view(batch_size, seq_len, model.config.n_head, d_head)\n",
    "        w = c_proj.weight.view(model.config.n_head, d_head, model.config.n_embd)\n",
    "\n",
    "        h_proj = einsum(\n",
    "            h, w,\n",
    "            \"batch_size seq_len n_head d_head, n_head d_head d_model -> batch_size seq_len n_head d_model\"\n",
    "        ).mean(0) # seq_len, n_head, d_model\n",
    "        h_proj = rearrange(h_proj, \"seq_len n_head d_model -> n_head seq_len d_model\")\n",
    "        \n",
    "        mean_head_activations[layer_idx] = h_proj\n",
    "        attn_layer_biases[layer_idx] = c_proj.bias\n",
    "\n",
    "    for layer in range(model.config.n_layer):\n",
    "        hook_fn = partial(_cache_mean_head_activations, layer_idx=layer)\n",
    "        hook = model.transformer.h[layer].attn.c_proj.register_forward_hook(hook_fn)\n",
    "        model(patching_tokens)\n",
    "        hook.remove()\n",
    "\n",
    "    return mean_head_activations, attn_layer_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_head_activations, attn_layer_biases = cache_mean_head_activations(model, patching_tokens)\n",
    "mean_attn_layer_activations = cache_mean_attn_layer_activations(model, patching_tokens)\n",
    "mean_mlp_activations = cache_mean_mlp_activations(model, patching_tokens)\n",
    "\n",
    "# SANITY CHECK: Do we obtain the same results?\n",
    "torch.allclose(mean_attn_layer_activations, mean_head_activations.sum(1) + attn_layer_biases[:, None, :], atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This module replaces the GPT2Attention layer and optionally outputs a bias term\n",
    "    \"\"\"\n",
    "    def __init__(self, bias):\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, hidden_states, **kwargs):\n",
    "        return (self.bias, None)\n",
    "\n",
    "\n",
    "class BiasLayerMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    This module replaces the GPT2MLP layer and optionally outputs a bias term\n",
    "    \"\"\"\n",
    "    def __init__(self, bias):\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, hidden_states, **kwargs):\n",
    "        return self.bias\n",
    "\n",
    "\n",
    "class AddLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This module replaces the GPT2Attention layer and optionally sums a bias term\n",
    "    \"\"\"\n",
    "    def __init__(self, attn: GPT2Attention, bias):\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        self.attn = attn\n",
    "\n",
    "    def forward(self, hidden_states, **kwargs):\n",
    "        output = self.attn(hidden_states)\n",
    "        return (output[0] + self.bias, None)\n",
    "\n",
    "\n",
    "class PassthroughLayer(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, hidden_states, **kwargs):\n",
    "        return (hidden_states, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, val_tokens, val_answer_tokens, task=\"acronyms\"):\n",
    "    if task == \"acronyms\":\n",
    "        return (model(val_tokens)[\"logits\"][:, -1].argmax(-1) == val_answer_tokens[:, -1]).float().mean().item()\n",
    "\n",
    "def list_to_dict(attn_heads, n_heads=12, n_layers=12):\n",
    "    \"\"\"\n",
    "    Given a list of the attn heads of the circuit, returns\n",
    "    a dictionary heads_to_prune[layer] = [head, ...] with\n",
    "    every attention head outside of the circuit.\n",
    "    \"\"\"\n",
    "    heads_to_prune = {}\n",
    "    for layer in range(n_layers):\n",
    "        heads_to_prune[layer] = [head for head in range(n_heads)]\n",
    "\n",
    "    for layer, head in attn_heads:\n",
    "        heads_to_prune[layer].remove(head)\n",
    "        \n",
    "    return heads_to_prune\n",
    "\n",
    "def get_attn_layers_to_prune(heads_to_prune, n_heads=12):\n",
    "    \"\"\"\n",
    "    If heads_to_prune[layer] contains every head of the attention layer,\n",
    "    we directly remove the complete layer instead of every separate head.\n",
    "    \"\"\"\n",
    "    attn_layers_to_prune = []\n",
    "    for layer in heads_to_prune.keys():\n",
    "        if len(heads_to_prune[layer]) == n_heads:\n",
    "            attn_layers_to_prune.append(layer)\n",
    "    for layer in attn_layers_to_prune:\n",
    "        del heads_to_prune[layer]\n",
    "    return heads_to_prune, attn_layers_to_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_parameters = (50257 * 768) + (1024 * 768)\n",
    "initial_parameters = model.num_parameters() - embedding_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8899999856948853, 85056000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(model, val_tokens, val_answer_tokens), initial_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, circuit_attn_heads):\n",
    "    #########################\n",
    "    #   PRUNE ATTENTION     #\n",
    "    #########################\n",
    "    heads_to_prune = list_to_dict(circuit_attn_heads)\n",
    "    heads_to_prune, attn_layers_to_prune = get_attn_layers_to_prune(heads_to_prune)\n",
    "    # Replace complete attn layers by just a bias term\n",
    "    for layer in attn_layers_to_prune:\n",
    "        model.transformer.h[layer].ln_1 = PassthroughLayer()\n",
    "        model.transformer.h[layer].attn = BiasLayer(bias=mean_attn_layer_activations[layer].cuda())\n",
    "    # Prune the individual heads \n",
    "    model.transformer._prune_heads(heads_to_prune)\n",
    "    # Add the bias term of the pruned heads to the respective attention layers\n",
    "    for layer in heads_to_prune.keys():\n",
    "        model.transformer.h[layer].attn = AddLayer(\n",
    "            attn=model.transformer.h[layer].attn,\n",
    "            bias=mean_head_activations[layer, heads_to_prune[layer]].sum(0).cuda()\n",
    "            )\n",
    "    \n",
    "    #################\n",
    "    #   PRUNE MLP   #\n",
    "    #################\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prune_model(model, gt_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit_mlps = [0, 1, 8, 9, 10, 11, 12]\n",
    "\n",
    "mlps_to_prune = [mlp for mlp in range(model.config.n_layer) if mlp not in circuit_mlps]\n",
    "# Replace MLPs\n",
    "for layer in mlps_to_prune:\n",
    "    model.transformer.h[layer].ln_2 = PassthroughLayer()\n",
    "    model.transformer.h[layer].mlp = BiasLayerMLP(bias=mean_mlp_activations[layer].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8499999642372131, 29938176)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(model, val_tokens, val_answer_tokens), model.num_parameters() - embedding_parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mech_interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
