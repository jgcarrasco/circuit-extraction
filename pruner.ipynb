{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgcarrasco/.virtualenvs/mech_interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "from experiments import prune_experiment, hyperparameter_experiments\n",
    "from utils import get_data, load_gpt2_tl\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# acronyms -> 250\n",
    "# ioi -> 150\n",
    "# greater-than -> 250\n",
    "n_patching = 250\n",
    "n_val = 250\n",
    "task = \"greater-than\"\n",
    "\n",
    "data = get_data(n_patching=n_patching, n_val=n_val, task=task)\n",
    "\n",
    "patching_tokens = data[\"patching_tokens\"].cuda()\n",
    "patching_answer_tokens = data[\"patching_answer_tokens\"].cuda() \n",
    "patching_logits = data[\"patching_logits\"]\n",
    "patching_cache = data[\"patching_cache\"]\n",
    "\n",
    "val_tokens = data[\"val_tokens\"].cuda()\n",
    "val_answer_tokens = data[\"val_answer_tokens\"].cuda()\n",
    "val_logits = data[\"val_logits\"]\n",
    "val_cache = data[\"val_cache\"]\n",
    "\n",
    "gt_circuit = data[\"gt_circuit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'patching_tokens' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhyperparameter_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43macronym\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/circuit-extraction/experiments.py:20\u001b[0m, in \u001b[0;36mhyperparameter_experiments\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m     17\u001b[0m n_patching \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m\n\u001b[1;32m     18\u001b[0m n_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m\n\u001b[0;32m---> 20\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_patching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_patching\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m patching_tokens \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatching_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     23\u001b[0m patching_cache \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatching_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Thesis/circuit-extraction/utils.py:250\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(n_patching, n_val, task)\u001b[0m\n\u001b[1;32m    246\u001b[0m     val_answer_tokens \u001b[38;5;241m=\u001b[39m tokens[n_patching:n_patching\u001b[38;5;241m+\u001b[39mn_val, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    248\u001b[0m     gt_circuit \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m9\u001b[39m], [\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m10\u001b[39m], [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m], [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m11\u001b[39m], [\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m1\u001b[39m]] \u001b[38;5;66;03m# we're only including attention heads, omitting MLPs for now\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m patching_logits, patching_cache \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun_with_cache(\u001b[43mpatching_tokens\u001b[49m)\n\u001b[1;32m    251\u001b[0m val_logits, val_cache \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun_with_cache(val_tokens)\n\u001b[1;32m    253\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatching_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m patching_tokens\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'patching_tokens' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "hyperparameter_experiments(\"acronym\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mech_interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
